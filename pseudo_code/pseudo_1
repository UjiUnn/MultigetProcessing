procedure ProcessQuery(pkt)
// packet header -> op, key-value, cutIndex, cutNum, keyNum
- hash() : hash function giving the server number
- cutIndex: tells where to cut the packet //0001011
- cutNum: tells number of truncated packets
- keyNum: tells the number of keys
- valueArr[][MAX]: store the value of the packet

    sort pkt.key by hash(pkt.key) from client

    //pkt.keyNum <= 4
    if pkt.op == mget1 then
            
        //Request
        if pkt.cutIndex < 0 then //1 001 -> 0010 1110
            left shift 1 pkt.cutIndex
            mirror the pkt to remPkt //mirror_packet
            recirculate the remPkt
            drop pkt.key[1...3] and pkt.value[1...3]
        
        else if pkt.cutIndex > 4 then //0101
            left shift 2 pkt.cutIndex
            mirror the pkt to remPkt
            recirculate the remPkt
            drop pkt.key[2...3] and pkt.value[2...3]

        else if pkt.cutIndex > 2 then //0011 
            left shift 3 pkt.cutIndex
            mirror the pkt to remPkt
            recirculate the remPkt
            drop pkt.key[3] and pkt.value[2]

        read(pkt.value) 
        
        //Reply
        if pkt arrived then
            if (pkt.cutNum - length of valueArr[pkt.id]) == 1 then
                add valueArr[pkt.id][0...pkt.cutNum-2] to pkt.value
                change pkt.dest to client IP address
                
            else valueArr[pkt.id][0...pkt.cutNum-2] = pkt.value

    //pkt.keyNum > 4
    else if pkt.op == mget2 then
    
        if pkt.keyNum > 4 then  0001 1101 -> 0000 0111
            mirror the pkt to remPkt 00000001
            right shift (pkt.keyNum - 4) pkt.cutIndex
            if pkt.cutIndex % 2 != 1 then
                temp = pkt.cutIndex || 0001
                drop pkt.cutIndex 
                add pkt.cutIndex (4 bits) to temp
            pkt.keyNum = 4
            drop pkt.key[4...pkt.keyNum] and pkt.value[4...pkt.keyNum]

            left shift 4 remPkt.cutIndex
            remPkt.keyNum = remPkt.keyNum - 4
            recirculate the remPkt

        pkt.op <- mget1
        recirculate the pkt

    else
        read(pkt.value)
        if pkt arrived then
            change pkt.dest to client IP address

    Forward packet to client

end procedure


- size of cutIndex bits in mget1 packet : 4 bits
- size of cutIndex bits in mget2 packet : MAX keyNum bits

// Q. pkt header를 추가하는게 효율적? 한 헤더에서 비트 수를 늘리는게 효율적??
// cutIndex를 이진수로 표현... 비트는 keyNum만큼 필요, 00000000 -> 끊으면 0, 안 끊으면 1로 표시... 00011101 -> 16+8+4+1 = 29
// cutIndex1, cutIndex2... 00001010
// mget이 나눠지면 cutIndex를 쪼갤 방법도 생각해봐야함... 어쨌든 쿼리 단위니까 여러개의 mget을 합치는 경우도 생각!! 이진수로 표현하면 시프트 연산으로 더 분할하기 쉽지않을까??

// Q. multiget request를 분리할 때 한번에 12개를 4 / 4 / 4 이렇게 나누는게 나은지, 아니면 4 / 8로 나눠서 8은 다시 recirculate 할 지??
// Q. packet header의 size를 변경 가능한가??

// 해결해야할 것 1. 스위치에서 read를 보낼 때 -> 클라이언트에서 먼저 해시하여 몇개 가는지 알려줌, 해시값 단위로 정렬
//               패킷을 받으면 정렬된 key1, key2...를 보고 스킵.. 스킵... 카운트를 세서 다 세면 해당 패킷은 미러링 후 뒤는 pkt.key 2...는 잘라서 서버에 보내버림, 그러면 전송 단에서는 패킷 저장할 필요없음
//            2. 서버에 read 할 때, mget쿼리가 여러개 들어오고 도착 순서가 보장 안됨 -> 패킷이 오면 [req.id][MAX]를 저장하는 배열을 만듦
//               해당 req의 마지막 패킷이 도착하면(이때 클라이언트에서 보내준 key 개수와 배열에 저장된 개수 비교) 해당 패킷의 개수 + 배열에 저장된 개수 비교해서 마지막이면 앞서 전송된 value를 패킷에 추가하여 클라이언트에게 전송
//               (순서 뒤바뀜은 어차피 패킷에 k1-v1 , k2-v2...이런식으로 저장되기 때문에 고려x)
//               저장 배열은 스위치 사이즈의 제한 때문에 많이 저장 못함... 평균 8.6개 키를 보내는데 스위치 스테이지는 12 ~ 20개 사이, 저장할 때마다 스테이지 소모임, 원래 스위치가 소모하는 스테이지도 있음
//               그래서 key의 개수가 4개 이하면 mget1으로 처리하고, 5개 이상이면 mget2로 처리하여 키의 개수를 분리할 수도 있음... -> 가정이니까 코디네이터 vs 분리해서 처리하기 어떤 성능이 나은지 추후에...
//              
// 알아야할 것 1. 스위치에서는 패킷만 처리하면 됨, 클라이언트에서 전처리 가능
//          2. 클라이언트 단위가 아니라 쿼리 단위로 해결하기
//          3. 스위치 사용 힌트는 패킷 헤더 필드 형태로 추가 가능
//              스위치는 패킷 단위로 처리됨(i.e., request 패킷과 reply 패킷을 동시에 처리한다던가 line-rate으로 자체적으로 패킷을 생성할 수 없음)
//          4. Request 단계에서는 key를 굳이 저장할 필요가 없다
//          5. Reply 단계에서는 Value를 저장해야한다
//          6. 하나의 메시지는 여러 패킷으로 구성될 수 있으나, Key-value store에서는 1 메시지 = 1 패킷
//          7. UDP 사용 가정 (재전송 없음)
//          8. multiget request가 포함하는 key의 갯수는 많을 수 있으나, 스위치의 제한된 stage 수로 인해 갯수가 제한 될 수 있음
//              -> multiget request 자체를 다시 분할하는 방법?
//          9. 수도코드 다듬고 -> P4로 스위치 프로그래밍 -> 클라이언트/서버 프로그래밍
//              
//             스위치의 stage 소모는 request + reply 둘 다,, 결국 client에서도 어디서 끊어야하는지 인덱스를 같이 보내주기
//              
//              